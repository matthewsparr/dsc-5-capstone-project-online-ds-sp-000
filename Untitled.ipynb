{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sparr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sparr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from queue import Queue, Empty\n",
    "from threading  import Thread\n",
    "from time import sleep\n",
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "import pickle\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enqueue_output(out, queue):\n",
    "    for line in iter(out.readline, b''):\n",
    "        queue.put(line)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(link):\n",
    "\n",
    "    return_links = []\n",
    "\n",
    "    r = requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(\"Error. Something is wrong here\")\n",
    "    else:\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(\"^\")}):\n",
    "            address = link.get('href')\n",
    "            if ('.z5' in address or '.z3' in address or '.z8' in address):\n",
    "                return_links.append('https://ifarchive.org/'+address[2:])\n",
    "    return return_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_links('https://ifarchive.org/indexes/if-archiveXgamesXzcode.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ifarchive.org//if-archive/games/zcode/7doctors.z5'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_games_and_grammar(links):\n",
    "    for url in links:\n",
    "        file_name = url.split('/')[-1]\n",
    "        r = requests.get(url, allow_redirects=True, stream=True)\n",
    "        f = open(file_name, 'wb')\n",
    "        f.write(r.content)\n",
    "        f.close()\n",
    "        \n",
    "        p = Popen(['infodump.exe', '-g', file_name], stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "        text = p.communicate()[0].decode('utf-8', errors='ignore').strip()\n",
    "        f = open('grammar/' + file_name[:-3] + '.txt', 'w')\n",
    "        f.write(text)\n",
    "        f.close()\n",
    "        p.terminate()\n",
    "        p.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_games_and_grammar(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('Action-Determiner-Object', None, [{POS: 'VERB'}, {POS: 'DET', 'OP':'*'}, {POS: 'ADJ', 'OP':'*'}, {POS: 'NOUN'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher2 = Matcher(nlp.vocab)\n",
    "matcher2.add('Action-Determiner-Object', None, [{POS: 'VERB'}, {POS: 'DET', 'OP':'*'}, {POS: 'ADJ', 'OP':'*'}, {POS: 'NOUN'},\n",
    "                                               {POS: 'ADP'}, {POS: 'DET', 'OP':'*'}, {POS: 'ADJ', 'OP':'*'}, {POS: 'NOUN'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "throw the rock at house "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'throw the rock at house '\n",
    "doc = nlp(string)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12505529351931920348, 0, 3)]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7% |#####                                                                   |\r"
     ]
    }
   ],
   "source": [
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "commands = []\n",
    "commands2 = []\n",
    "directory = os.fsencode(os.getcwd()+'\\\\grammar')\n",
    "for link in pbar(links):\n",
    "    file_name = link.split('/')[-1]\n",
    "    text_file = file_name[:-3] + '.txt'\n",
    "    f = open('grammar/' + text_file, 'r')\n",
    "    text = f.read().strip()\n",
    "    text = re.sub(r'([^\\s\\w]|_)+', '', text)\n",
    "    if (len(text) > 900000):\n",
    "        text = text[0:800000]\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches2 = matcher2(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        #verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        #text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands.append(span)\n",
    "    for match_id, start, end in matches2:\n",
    "        span = doc[start:end]\n",
    "        #verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        #text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands2.append(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[noscript synonyms,\n",
       " carry inventory,\n",
       " carry noun,\n",
       " carry noun,\n",
       " using noun,\n",
       " pick multi,\n",
       " remove noun,\n",
       " remove noun,\n",
       " using noun,\n",
       " put multiexcept,\n",
       " put multiexcept,\n",
       " put multiheld,\n",
       " discard multiexcept,\n",
       " give offer,\n",
       " go synonyms,\n",
       " leave noun,\n",
       " consult noun,\n",
       " uncover undo,\n",
       " describe examine,\n",
       " read noun,\n",
       " read topic,\n",
       " adjust noun,\n",
       " adjust noun,\n",
       " adjust noun,\n",
       " drag noun,\n",
       " drag noun,\n",
       " turn twist,\n",
       " rotate noun,\n",
       " rotate noun,\n",
       " break crack,\n",
       " hit kill,\n",
       " wait synonyms,\n",
       " say shout,\n",
       " tell creature,\n",
       " ask creature,\n",
       " ask creature,\n",
       " hear synonyms,\n",
       " hear noun,\n",
       " feel synonyms,\n",
       " burn synonyms,\n",
       " burn noun,\n",
       " burn noun,\n",
       " cut prune slice,\n",
       " pour noun,\n",
       " pour noun,\n",
       " fill noun,\n",
       " fill noun,\n",
       " carry inventory,\n",
       " carry noun,\n",
       " carry noun,\n",
       " using noun,\n",
       " remove noun,\n",
       " using noun,\n",
       " put multiheld,\n",
       " discard multiheld,\n",
       " put multiexcept,\n",
       " discard multiexcept,\n",
       " put multiexcept,\n",
       " insert multiexcept,\n",
       " discard multiexcept,\n",
       " leave noun,\n",
       " check noun,\n",
       " rotate noun,\n",
       " switch noun,\n",
       " rotate noun,\n",
       " switch noun,\n",
       " burn noun,\n",
       " burn noun,\n",
       " hear noun,\n",
       " using noun,\n",
       " attach noun,\n",
       " fill noun,\n",
       " bother topic,\n",
       " rotate noun,\n",
       " tell creature,\n",
       " ask creature,\n",
       " consult noun,\n",
       " consult noun,\n",
       " read topic,\n",
       " ask creature,\n",
       " pour noun,\n",
       " drag noun,\n",
       " rotate noun,\n",
       " adjust noun,\n",
       " adjust noun low,\n",
       " adjust noun,\n",
       " pour noun,\n",
       " fill noun]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[carry noun with noun,\n",
       " remove noun with noun,\n",
       " consult noun on topic,\n",
       " read topic in noun,\n",
       " drag noun under noun,\n",
       " tell creature about topic,\n",
       " ask creature about topic,\n",
       " ask creature for noun,\n",
       " pour noun into noun,\n",
       " fill noun from noun,\n",
       " carry noun with noun,\n",
       " attach noun to noun,\n",
       " tell creature about topic,\n",
       " ask creature about topic,\n",
       " consult noun about topic,\n",
       " consult noun on topic,\n",
       " read topic in noun,\n",
       " ask creature for noun,\n",
       " pour noun into noun,\n",
       " drag noun under noun]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commands2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commands2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
