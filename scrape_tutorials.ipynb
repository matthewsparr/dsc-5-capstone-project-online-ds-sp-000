{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sparr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sparr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import string\n",
    "import pickle\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import POS\n",
    "import spacy\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_tutorial_text():\n",
    "    tutorials = open('tutorials.txt', 'w')\n",
    "    return tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tutorial(url):\n",
    "    #text = urllib.request.urlopen(url).read()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    texts = soup.findAll(text=True, strip=True)\n",
    "    x = \"\"\n",
    "    for i in texts:\n",
    "        x = x + i\n",
    "    req = urllib.request.Request(url,headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "    text = urllib.request.urlopen(req).read().decode('utf-8')\n",
    "    text = re.sub(r'([^\\s\\w]|_)+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tutorial_to_file(tutorials, text):\n",
    "    tutorials.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(link, site):\n",
    "\n",
    "    return_links = []\n",
    "\n",
    "    r = requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(\"Error. Something is wrong here\")\n",
    "    else:\n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(\"^\")}):\n",
    "            address = link.get('href')\n",
    "            if ('.txt' in address):\n",
    "                if (site == 1):\n",
    "                    return_links.append('https://www.ifarchive.org/' + address.replace('../../../', ''))\n",
    "                else:\n",
    "                    return_links.append('http://www.textfiles.com/adventure/' + address)\n",
    "    return return_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_verb(token):\n",
    "    \"\"\"Check verb type given spacy token\"\"\"\n",
    "    if token.pos_ == 'VERB':\n",
    "        indirect_object = False\n",
    "        direct_object = False\n",
    "        for item in token.children:\n",
    "            if(item.dep_ == \"iobj\" or item.dep_ == \"pobj\"):\n",
    "                indirect_object = True\n",
    "            if (item.dep_ == \"dobj\" or item.dep_ == \"dative\"):\n",
    "                direct_object = True\n",
    "        if indirect_object and direct_object:\n",
    "            return 'DITRANVERB'\n",
    "        elif direct_object and not indirect_object:\n",
    "            return 'TRANVERB'\n",
    "        elif not direct_object and not indirect_object:\n",
    "            return 'INTRANVERB'\n",
    "        else:\n",
    "            return 'VERB'\n",
    "    else:\n",
    "        return token.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = (get_links(\"https://www.ifarchive.org/indexes/if-archive/solutions/\", 1) + \n",
    "get_links(\"http://www.textfiles.com/adventure/\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('Action-Determiner-Object', None, [{POS: 'VERB'}, {POS: 'DET'}, {POS: 'ADJ', 'OP':'*'}, {POS: 'NOUN'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher2 = Matcher(nlp.vocab)\n",
    "matcher2.add('Action-Determiner-Object', None, [{POS: 'VERB'}, {POS: 'DET'}, {POS: 'ADJ', 'OP':'*'}, {POS: 'NOUN'},\n",
    "                                               {POS: 'ADP'}, {POS: 'DET'}, {POS: 'ADJ', 'OP':'*'}, {POS: 'NOUN'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = []\n",
    "commands2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    try:\n",
    "        text = download_tutorial(links[i])\n",
    "    except:\n",
    "        print(\"file didn't work\")\n",
    "        pass\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches2 = matcher2(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands.append(text)\n",
    "    for match_id, start, end in matches2:\n",
    "        span = doc[start:end]\n",
    "        verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n"
     ]
    }
   ],
   "source": [
    "for i in range(101,200):\n",
    "    try:\n",
    "        text = download_tutorial(links[i])\n",
    "    except:\n",
    "        print(\"file didn't work\")\n",
    "        pass\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches2 = matcher2(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands.append(text)\n",
    "    for match_id, start, end in matches2:\n",
    "        span = doc[start:end]\n",
    "        verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n",
      "file didn't work\n"
     ]
    }
   ],
   "source": [
    "for i in range(201,305):\n",
    "    try:\n",
    "        text = download_tutorial(links[i])\n",
    "    except:\n",
    "        print(\"file didn't work\")\n",
    "        pass\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches2 = matcher2(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands.append(text)\n",
    "    for match_id, start, end in matches2:\n",
    "        span = doc[start:end]\n",
    "        verb = WordNetLemmatizer().lemmatize(str(nlp(span.text)[0]),'v')\n",
    "        text = verb + \" \" + str(nlp(span.text)[1:])\n",
    "        commands2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13554, 1530)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(commands), len(commands2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('commands.txt', 'wb') as cmd:\n",
    "    pickle.dump(commands, cmd)\n",
    "with open('commands2.txt', 'wb') as cmd2:\n",
    "    pickle.dump(commands2, cmd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
